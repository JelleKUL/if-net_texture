{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good examples:\n",
    "\n",
    "-projects/if-net_texture/experiments/shapenet_channel/evaluation_278/bf9ea87a9765eed7d53b73fe621a84b4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "The input dataset currently goes through multiple pre processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vertices', 'faces', 'normals', 'texcoords', 'texcoords_indices', 'texture', 'vertex_colors', 'face_vertices']\n",
      "vertices\n",
      "(9968, 3)\n",
      "faces\n",
      "(20000, 3)\n",
      "normals\n",
      "()\n",
      "texcoords\n",
      "(13857, 2)\n",
      "texcoords_indices\n",
      "(20000, 3)\n",
      "texture\n",
      "(2048, 2048, 3)\n",
      "vertex_colors\n",
      "()\n",
      "face_vertices\n",
      "(106,)\n"
     ]
    }
   ],
   "source": [
    "# step -1: convert the obj to a compressed numpy array\n",
    "import numpy as np\n",
    "\n",
    "filePath = \"dataset/SHARP2020/challenge1-track1/test/170410-001-a-r9iu-494a-low-res-result/170410-001-a-r9iu-494a-low-res-result_normalized.npz\"\n",
    "\n",
    "b = np.load(filePath, allow_pickle=True)\n",
    "\n",
    "print(list(b.keys()))\n",
    "\n",
    "for key in b.keys():\n",
    "    print(key)\n",
    "    print(b[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['points', 'grid_coords', 'colors']\n",
      "points\n",
      "[[ 0.12064141  0.88453791 -0.24881006]\n",
      " [-0.1544254   0.68709336 -0.25824036]\n",
      " [-0.18368903  0.25557288 -0.13980684]\n",
      " ...\n",
      " [-0.38154646  1.22621116 -0.16947448]\n",
      " [ 0.00948102  1.06471689 -0.31133876]\n",
      " [-0.4635408   0.98523597 -0.12528596]]\n",
      "[-0.50958245 -0.50924814 -0.50912306 ...  1.78317587  1.78319532\n",
      "  1.78338817]\n",
      "grid_coords\n",
      "[[-0.03554429 -0.8205202   0.01723449]\n",
      " [-0.03689148 -0.83932444 -0.02206077]\n",
      " [-0.01997241 -0.88042163 -0.02624129]\n",
      " ...\n",
      " [-0.02421064 -0.78797989 -0.05450664]\n",
      " [-0.04447697 -0.8033603   0.00135443]\n",
      " [-0.01789799 -0.81092991 -0.06622011]]\n",
      "[-0.9038141  -0.903814   -0.90381344 ...  0.04831714  0.04831819\n",
      "  0.04835757]\n",
      "colors\n",
      "[[44 50 59]\n",
      " [44 58 69]\n",
      " [43 66 83]\n",
      " ...\n",
      " [41 62 81]\n",
      " [29 52 94]\n",
      " [49 54 68]]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 217\n",
      " 221 223 226 227 231 236 242 248 252]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path = 'projects/if-net_texture/dataset/SHARP2020/chairs/test/170410-001-a-r9iu-494a-low-res-result/170410-001-a-r9iu-494a-low-res-result_normalized_color_samples100000_bbox-7,7,-1,20,-7,7.npz'\n",
    "split = np.load(path[24:])\n",
    "print(split.files)\n",
    "\n",
    "for key in split.files:\n",
    "    print(key)\n",
    "    print(split[key])\n",
    "    print(np.unique(split[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.local_model as model\n",
    "import models.dataloader as dataloader\n",
    "import argparse\n",
    "from models.generation import Generator\n",
    "import config.config_loader as cfg_loader\n",
    "import trimesh\n",
    "import torch\n",
    "from data_processing import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv3d(4, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), padding_mode=reflect)\n",
      "Loaded checkpoint from: /home/jvermandere/projects/if-net_texture/models/../experiments/SHARP2020_c1_t1/checkpoints/checkpoint_epoch_99.tar\n"
     ]
    }
   ],
   "source": [
    "cfg = cfg_loader.load(\"config/SHARP2020/track1.yaml\")\n",
    "net = model.get_models()[cfg['model']]()\n",
    "gen = Generator(net, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = \"dataset/SHARP2020/challenge1-track1/test/170410-001-a-r9iu-494a-low-res-result/170410-001-a-r9iu-494a-low-res-result_normalized-partial-00_voxelized_colored_point_cloud_res128_points100000_bbox-0.8,0.8,-0.15,2.1,-0.8,0.8.npz\"\n",
    "inputResolution = cfg['input_resolution']\n",
    "\n",
    "R = np.load(path)['R']\n",
    "G = np.load(path)['G']\n",
    "B = np.load(path)['B']\n",
    "S = np.load(path)['S']\n",
    "\n",
    "R = np.reshape(R, (inputResolution,)*3)\n",
    "G = np.reshape(G, (inputResolution,)*3)\n",
    "B = np.reshape(B, (inputResolution,)*3)\n",
    "S = np.reshape(S, (inputResolution,)*3)\n",
    "input = np.array([R,G,B,S])\n",
    "\n",
    "inputs = torch.from_numpy(np.array(input, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grid_sampler(): expected grid and input to have same batch size, but got input with sizes [4, 128, 128, 128] and grid with sizes [1, 1, 7, 159968, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jvermandere/projects/if-net_texture/textureprediction.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.130.0.46/home/jvermandere/projects/if-net_texture/textureprediction.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m pred_verts_gird_coords \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_grid_sample_coords( pred_mesh\u001b[39m.\u001b[39mvertices, cfg[\u001b[39m'\u001b[39m\u001b[39mdata_bounding_box\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.130.0.46/home/jvermandere/projects/if-net_texture/textureprediction.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m pred_verts_gird_coords \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(pred_verts_gird_coords)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.130.0.46/home/jvermandere/projects/if-net_texture/textureprediction.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m colors_pred_surface \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49mgenerate_colors(inputs, pred_verts_gird_coords)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.130.0.46/home/jvermandere/projects/if-net_texture/textureprediction.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# attach predicted colors to the mesh\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.130.0.46/home/jvermandere/projects/if-net_texture/textureprediction.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m pred_mesh\u001b[39m.\u001b[39mvisual\u001b[39m.\u001b[39mvertex_colors \u001b[39m=\u001b[39m colors_pred_surface\n",
      "File \u001b[0;32m~/projects/if-net_texture/models/generation.py:50\u001b[0m, in \u001b[0;36mGenerator.generate_colors\u001b[0;34m(self, inputs, points)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m p_batch \u001b[39min\u001b[39;00m p_batches:\n\u001b[1;32m     49\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 50\u001b[0m         pred_rgb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(p_batch,i)\n\u001b[1;32m     51\u001b[0m     full_pred\u001b[39m.\u001b[39mappend(pred_rgb\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[1;32m     53\u001b[0m pred_rgb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(full_pred, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.conda/envs/if-net/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/if-net_texture/models/local_model.py:354\u001b[0m, in \u001b[0;36mTEXR.forward\u001b[0;34m(self, p, x)\u001b[0m\n\u001b[1;32m    352\u001b[0m p \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    353\u001b[0m p \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([p \u001b[39m+\u001b[39m d \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisplacments], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> 354\u001b[0m feature_0 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mgrid_sample(x, p, padding_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mborder\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    355\u001b[0m \u001b[39m# print(feature_0.shape)\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m# print(feature_0[:,:,:,0,0])\u001b[39;00m\n\u001b[1;32m    358\u001b[0m net \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactvn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_in(x))\n",
      "File \u001b[0;32m~/.conda/envs/if-net/lib/python3.10/site-packages/torch/nn/functional.py:4244\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4236\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   4237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4240\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4241\u001b[0m     )\n\u001b[1;32m   4242\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 4244\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grid_sampler(): expected grid and input to have same batch size, but got input with sizes [4, 128, 128, 128] and grid with sizes [1, 1, 7, 159968, 3]"
     ]
    }
   ],
   "source": [
    "meshPath = \"dataset/SHARP2020/challenge1-track1/test/170410-001-a-r9iu-494a-low-res-result/170410-001-a-r9iu-494a-low-res-result_normalized.obj\"\n",
    "mesh = trimesh.load(meshPath)\n",
    "        \n",
    "# create new uncolored mesh for color prediction\n",
    "pred_mesh = trimesh.Trimesh(mesh.vertices, mesh.faces)\n",
    "\n",
    "# colors will be attached per vertex\n",
    "# subdivide in order to have high enough number of vertices for good texture representation\n",
    "pred_mesh = pred_mesh.subdivide().subdivide()\n",
    "\n",
    "pred_verts_gird_coords = utils.to_grid_sample_coords( pred_mesh.vertices, cfg['data_bounding_box'])\n",
    "pred_verts_gird_coords = torch.tensor(pred_verts_gird_coords).unsqueeze(0)\n",
    "\n",
    "colors_pred_surface = gen.generate_colors(inputs, pred_verts_gird_coords)\n",
    "\n",
    "# attach predicted colors to the mesh\n",
    "pred_mesh.visual.vertex_colors = colors_pred_surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.13770886  0.099706   -0.12823488]\n",
      " [ 0.1605673   0.11850667 -0.20837104]\n",
      " [ 0.1467137   0.24583559 -0.23430079]\n",
      " ...\n",
      " [-0.27044937  0.9719703  -0.14337961]\n",
      " [-0.12032622  1.042221    0.00904251]\n",
      " [ 0.03044558  1.0324756  -0.30844074]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load(\"dataset/SHARP2020/challenge1-track1/test/170410-001-a-r9iu-494a-low-res-result/170410-001-a-r9iu-494a-low-res-result_normalized-partial-01.npz\")\n",
    "print(data['vertices'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tex_if-net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
